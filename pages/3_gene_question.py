# __import__("pysqlite3")
import sys

# sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import os
import re
import time
import json
import requests

import streamlit as st
from langchain.chains.llm import LLMChain
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_openai import ChatOpenAI
from PIL import Image
from streamlit_extras.switch_page_button import switch_page

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from src.generate_question import create_prompt_with_jd
from src.generate_question import (create_prompt_with_question,
                                   create_prompt_with_resume,
                                   create_resume_vectordb, load_user_JD,
                                   load_user_resume, save_user_JD,
                                   save_user_resume)
from src.rule_based import list_extend_questions_based_on_keywords
from src.util import local_css, read_prompt_from_txt
from src.semantic_search import faiss_inference, reranker
from config import DATA_DIR, IMG_PATH, CSS_PATH, PORT, MODEL_NAME

st.session_state["FAV_IMAGE_PATH"] = os.path.join(IMG_PATH, "favicon.png")
st.set_page_config(
    page_title="Hello Jobits",  # ë¸Œë¼ìš°ì €íƒ­ì— ëœ° ì œëª©
    page_icon=Image.open(
        st.session_state.FAV_IMAGE_PATH
    ),  # ë¸Œë¼ìš°ì € íƒ­ì— ëœ° ì•„ì´ì½˜,Image.open ì„ ì´ìš©í•´ íŠ¹ì •ê²½ë¡œ ì´ë¯¸ì§€ ë¡œë“œ
    layout="wide",
    initial_sidebar_state="collapsed",)

st.session_state.logger.info("start")
NEXT_PAGE = "show_questions_hint"

MY_PATH = os.path.dirname(os.path.dirname(__file__))

#### style css ####


local_css(os.path.join(CSS_PATH, "background.css"))
local_css(os.path.join(CSS_PATH, "2_generate_question.css"))


st.markdown(f"""
            <style>
                .main {{
                    background-color: #99A7E4; /* ë°°ê²½ìƒ‰ */
                    }}
                /* ë¡œë”©ì´ë¯¸ì§€ */
                .loading_space {{
                    display : flex;
                    justify-content : center;
                    padding-top : 4rem;
                }}
                .loading_space img{{
                    max-width : 70%;
                }}
                .loading_text {{
                    padding-top : 2rem;
                    z-index : 99;
                }}
                .loading_text p{{
                    font-family : 'Nanumsquare';
                    color:#4C4F6D;
                    font-size:28px ;
                    line-height:1.5;
                    word-break:keep-all;
                    font-weight:700;
                    text-align:center;
                    z-index : 99;
                }}
            </style>
            """,unsafe_allow_html=True)


## set save dir
USER_RESUME_SAVE_DIR = os.path.join(st.session_state["save_dir"], "2_generate_question_user_resume.pdf")
USER_JD_SAVE_DIR = os.path.join(st.session_state["save_dir"], "2_generate_question_user_JD.txt")
BIG_QUESTION_SAVE_DIR = os.path.join(st.session_state["save_dir"], "2_generate_question_generated_big_question.txt")

# ì§„í–‰ë¥ 
progress_holder = st.empty()  # ì‘ì—…ì— ë”°ë¼ ë¬¸êµ¬ ë°”ë€ŒëŠ” ê³³
loading_message = [
    f" JOBits ê°€ '{st.session_state.user_name}'ë‹˜ì˜ ë©´ì ‘ ì§ˆë¬¸ì„ ì„ ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤ğŸ§ <br> 2~3ë¶„ ë‚´ì™¸ë¡œ ìƒì„±ë©ë‹ˆë‹¤ğŸ¤— ",
    f"'{st.session_state.user_name}'ë‹˜ì´ ë©´ì ‘ì¥ì— ë“¤ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤ğŸš¶ğŸ»â€â™€ï¸ğŸš¶ğŸ»",
]

## ë©´ì ‘&ì´ë ¥ì„œ íŒ
## ê³µê°„ì´ì ì´ë¯¸ì§€ê°€ ë“¤ì–´ê°€ë©´ ì¢‹ì„ ê²ƒ ê°™ì€ ê³³
st.markdown(f"""
            <div class='loading_space'>
            <img class='tips' src="data:image/gif;base64,{st.session_state['LOADING_GIF1']}"></div>""",
            unsafe_allow_html=True,
            )

with progress_holder:
    ### step1 : ëŒ€ì§ˆë¬¸ ìƒì„± ë° ì¶”ì¶œ, step2 : ì „ì²˜ë¦¬(time.sleep(3))
    progress_holder.markdown(f"""
                                <div class="loading_text">
                                <p>{loading_message[0]}</p></div>""",
                                unsafe_allow_html=True,
                                )

    # ëŒ€ì§ˆë¬¸ ìƒì„± #

    ### ì´ë ¥ì„œ Pre-process
    st.session_state.logger.info("resume process ")
    ### uploaded_fileëŠ” streamlitì˜ file_uploaderì—ì„œ ë°˜í™˜ëœ ê°ì²´
    user_resume = st.session_state['user_email'] + 'uploaded_resume'
    st.session_state.uploaded_file_resume = st.session_state[user_resume]
    ### ì €ì¥
    save_user_resume(USER_RESUME_SAVE_DIR, st.session_state.uploaded_file_resume)
    st.session_state.logger.info("save resume")
    ### ë¶ˆëŸ¬ì˜¤ê¸°
    user_resume = load_user_resume(USER_RESUME_SAVE_DIR)
    st.session_state.logger.info("user total resume import")

    ### JD Pre-process ########################################
    st.session_state.logger.info("JD precess")
    ### uploaded_txt ë¡œ uploaded_JD ì—ì„œ JD ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤
    user_jd = st.session_state['user_email'] + 'uploaded_JD'
    st.session_state.uploaded_file_jd = st.session_state[user_jd]
    ### ì €ì¥ USER_JD_SAVE_DIR ê²½ë¡œì— uploaded_file ë‚´ìš©ì„ ì ì–´ ì €ì¥í•©ë‹ˆë‹¤.
    save_user_JD(USER_JD_SAVE_DIR, st.session_state.uploaded_file_jd)
    st.session_state.logger.info("save JD")
    ### ë¶ˆëŸ¬ì˜¤ê¸°
    st.session_state.user_JD = load_user_JD(USER_JD_SAVE_DIR)
    st.session_state.logger.info("user total JD import")

    ### JD ì‚¬ìš©í•˜ì—¬ JD ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°
    st.session_state.logger.info("prompt JD start")
    prompt_template_jd = read_prompt_from_txt(os.path.join(DATA_DIR, "test", "prompt_JD_template.txt"))
    st.session_state.prompt_JD = create_prompt_with_jd(prompt_template_jd)
    
    # prompt_JD ìƒì„±ì™„ë£Œ
    st.session_state.logger.info("create prompt JD object")

    ### ëª¨ë¸ ì„¸íŒ… ê·¸ëŒ€ë¡œ
    llm = ChatOpenAI(temperature=st.session_state.temperature, model_name=MODEL_NAME)

    st.session_state.logger.info("create llm object")

    ######## ì´ì œ íƒœì—°ìŠ¤ì˜  ë°ëª¨ë¥¼ ì²´ì¸ìœ¼ë¡œ ë°”ê¿” ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.
    # STEP 1. ì‚¬ìš©ìê°€ ì…ë ¥í•œ JD ë¥¼ GPT ë¥¼ ì´ìš©í•´ job_description ì„ ë½‘ìŠµë‹ˆë‹¤.

    # ì‚¬ìš© ì‹œê°„ ì¶œë ¥ìš©
    start = time.time()
    ###################
    st.session_state.chain_JD_1 = LLMChain(llm=llm, prompt=st.session_state.prompt_JD)
    st.session_state.logger.info("create chain_JD_1 object")
    st.session_state.job_description = st.session_state.chain_JD_1.invoke(st.session_state.user_JD)['text']
    st.session_state.logger.info("chain_JD_1 complit")

    # STEP 2. step 1 ì—ì„œ ìƒì„±ëœ job_description ë¥¼ qa prompt template ì— ë„£ê³ , GPT ì— ì§ˆì˜í•˜ì—¬ ì˜ˆìƒ ì§ˆë¬¸ì„ ë½‘ìŠµë‹ˆë‹¤.
    # prompt_qa_template #######################################

    st.session_state.logger.info("prompt resume start")
    prompt_template_resume = read_prompt_from_txt(os.path.join(DATA_DIR, "test", "prompt_resume_template.txt"))

    st.session_state.logger.info("create prompt resume template")
    st.session_state.prompt_resume = create_prompt_with_resume(prompt_template_resume)

    st.session_state.logger.info("create prompt_resume")
    st.session_state.vector_index = create_resume_vectordb(USER_RESUME_SAVE_DIR)  # ì´ë ¥ì„œ vectordbë¥¼ ìƒì„±í•´ì¤ë‹ˆë‹¤.

    st.session_state.logger.info("user_resume chunk OpenAIEmbeddings ")

    ### STEP 2 ë¥¼ ìœ„í•œ ìƒˆ ëª¨ë¸ í˜¸ì¶œ
    llm2 = ChatOpenAI(temperature=0.0, model_name=MODEL_NAME)

    st.session_state.chain_type_kwargs = {"prompt": st.session_state.prompt_resume}

    st.session_state.qa_chain = RetrievalQA.from_chain_type(llm=llm2,
                                                            chain_type="stuff",
                                                            retriever=st.session_state.vector_index.as_retriever(),
                                                            chain_type_kwargs=st.session_state.chain_type_kwargs,
                                                            verbose=True,)

    st.session_state.resume = st.session_state.qa_chain.invoke("ê¸°ìˆ ë©´ì ‘ì— ë‚˜ì˜¬ë§Œí•œ í”„ë¡œì íŠ¸ ë‚´ìš©ì€?")['result']
    st.session_state.logger.info(" prompt_resume running complit")
    print("ì‚¬ìš©ì", st.session_state.user_email, "ì˜ resume : \n", st.session_state.resume)

    st.session_state.vector_index.delete_collection()

    ## step3 :
    st.session_state.logger.info("prompt question start")
    prompt_template_question = read_prompt_from_txt(os.path.join(DATA_DIR, "test", "prompt_question_template.txt"))

    st.session_state.logger.info("create prompt question template")
    st.session_state.prompt_question = create_prompt_with_question(prompt_template_question)

    llm3 = ChatOpenAI(temperature=0, model_name=MODEL_NAME)
    st.session_state.chain = LLMChain(llm=llm3, prompt=st.session_state.prompt_question)
    st.session_state.main_question = st.session_state.chain.invoke({"jd": st.session_state.job_description, "resume": st.session_state.resume})['text']
    #################
    end = time.time()
    st.session_state.logger.info(
        f"generate big question run time is ... {(end-start)/60:.3f} ë¶„ ({(end-start):0.1f}ì´ˆ)"
    )
    st.session_state.logger.info(" prompt_quest running complete")
    print(st.session_state.main_question)

    ### STEP 3. ê²°ê³¼ë¬¼ ë° Token ì‚¬ìš©ëŸ‰ ì €ì¥
    ### ê²°ê³¼ í…ìŠ¤íŠ¸ ì €ì¥
    # '\n\n'ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ë¶„ë¦¬ í›„ ë°”ë¡œ unpacking
    
    # ê° í•­ëª©ì„ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    st.session_state.main_question = re.split(r"\n\d+\.\s*", st.session_state.main_question.strip())
    # ì²« ë²ˆì§¸ ë¹ˆ í•­ëª© ì œê±°
    st.session_state.main_question = [question for question in st.session_state.main_question if question]
    st.session_state.logger.info(f"save question result")


    ### User pdfíŒŒì¼ ì‚­ì œ
    # try:
    #     os.remove(USER_RESUME_SAVE_DIR)
    # except Exception as e:
    #     st.session_state.logger.info(f"User resume delete Error: \n{e}")
    #     print(">>> User resume delete Error: \n{e}")
    
    progress_holder.markdown(f"""
                                <div class="loading_text">
                                <p>{loading_message[1]}</p></div>""",
                                unsafe_allow_html=True,
                                )
    # ì§ˆë¬¸ ìƒì„± # 
    st.session_state.logger.info("end gene_question")
    
    time.sleep(2)
    ####
    from requests_toolbelt.multipart.encoder import MultipartEncoder
    # ê¸°ë¡ ì €ì¥ ìš”ì²­ ë³´ë‚´ê¸°
    # ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ ì •ì˜
    url = f"http://localhost:{PORT}/users/records/{st.session_state.user_email}"
    jd = st.session_state.user_JD
    filename = st.session_state.uploaded_file_resume.name
    file_path = USER_RESUME_SAVE_DIR
    
    headers = {
        "accept": "application/json",
    }
    data = {
        'jd': jd,
        'questions': "\n".join(st.session_state.main_question),
        'filename': filename,
    }
    files = {
        'file_data': (filename, open(USER_RESUME_SAVE_DIR, "rb"), 'application/pdf')
    }
    
    response = requests.post(url, headers=headers, data=data, files=files)

    # ì‘ë‹µ í™•ì¸ìš© ì½”ë“œ
    # import json
    # response_data = json.loads(response.text)

    if st.session_state.cur_task == 'gene_question':
        switch_page('show_questions_hint')
    elif st.session_state.cur_task == 'interview':
        switch_page('interview')
